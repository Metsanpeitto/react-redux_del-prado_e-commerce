{"id":"../node_modules/webpack/node_modules/@webassemblyjs/wast-parser/esm/grammar.js","dependencies":[{"name":"/home/jarri/Desktop/lenovo2020/projects-web/eCommerce/template-store1/package.json","includedInParent":true,"mtime":1594655165833},{"name":"/home/jarri/Desktop/lenovo2020/projects-web/eCommerce/template-store1/node_modules/webpack/node_modules/@webassemblyjs/wast-parser/package.json","includedInParent":true,"mtime":1594578778157},{"name":"@webassemblyjs/helper-code-frame","loc":{"line":5,"column":36},"parent":"/home/jarri/Desktop/lenovo2020/projects-web/eCommerce/template-store1/node_modules/webpack/node_modules/@webassemblyjs/wast-parser/esm/grammar.js","resolved":"/home/jarri/Desktop/lenovo2020/projects-web/eCommerce/template-store1/node_modules/webpack/node_modules/@webassemblyjs/helper-code-frame/esm/index.js"},{"name":"@webassemblyjs/ast","loc":{"line":6,"column":19},"parent":"/home/jarri/Desktop/lenovo2020/projects-web/eCommerce/template-store1/node_modules/webpack/node_modules/@webassemblyjs/wast-parser/esm/grammar.js","resolved":"/home/jarri/Desktop/lenovo2020/projects-web/eCommerce/template-store1/node_modules/webpack/node_modules/@webassemblyjs/ast/esm/index.js"},{"name":"./number-literals","loc":{"line":7,"column":25},"parent":"/home/jarri/Desktop/lenovo2020/projects-web/eCommerce/template-store1/node_modules/webpack/node_modules/@webassemblyjs/wast-parser/esm/grammar.js","resolved":"/home/jarri/Desktop/lenovo2020/projects-web/eCommerce/template-store1/node_modules/webpack/node_modules/@webassemblyjs/wast-parser/esm/number-literals.js"},{"name":"./string-literals","loc":{"line":8,"column":28},"parent":"/home/jarri/Desktop/lenovo2020/projects-web/eCommerce/template-store1/node_modules/webpack/node_modules/@webassemblyjs/wast-parser/esm/grammar.js","resolved":"/home/jarri/Desktop/lenovo2020/projects-web/eCommerce/template-store1/node_modules/webpack/node_modules/@webassemblyjs/wast-parser/esm/string-literals.js"},{"name":"./tokenizer","loc":{"line":9,"column":33},"parent":"/home/jarri/Desktop/lenovo2020/projects-web/eCommerce/template-store1/node_modules/webpack/node_modules/@webassemblyjs/wast-parser/esm/grammar.js","resolved":"/home/jarri/Desktop/lenovo2020/projects-web/eCommerce/template-store1/node_modules/webpack/node_modules/@webassemblyjs/wast-parser/esm/tokenizer.js"}],"generated":{"js":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.parse = parse;\n\nvar _helperCodeFrame = require(\"@webassemblyjs/helper-code-frame\");\n\nvar t = _interopRequireWildcard(require(\"@webassemblyjs/ast\"));\n\nvar _numberLiterals = require(\"./number-literals\");\n\nvar _stringLiterals = require(\"./string-literals\");\n\nvar _tokenizer = require(\"./tokenizer\");\n\nfunction _getRequireWildcardCache() { if (typeof WeakMap !== \"function\") return null; var cache = new WeakMap(); _getRequireWildcardCache = function () { return cache; }; return cache; }\n\nfunction _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } if (obj === null || typeof obj !== \"object\" && typeof obj !== \"function\") { return { default: obj }; } var cache = _getRequireWildcardCache(); if (cache && cache.has(obj)) { return cache.get(obj); } var newObj = {}; var hasPropertyDescriptor = Object.defineProperty && Object.getOwnPropertyDescriptor; for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) { var desc = hasPropertyDescriptor ? Object.getOwnPropertyDescriptor(obj, key) : null; if (desc && (desc.get || desc.set)) { Object.defineProperty(newObj, key, desc); } else { newObj[key] = obj[key]; } } } newObj.default = obj; if (cache) { cache.set(obj, newObj); } return newObj; }\n\nfunction _typeof(obj) {\n  if (typeof Symbol === \"function\" && typeof Symbol.iterator === \"symbol\") {\n    _typeof = function _typeof(obj) {\n      return typeof obj;\n    };\n  } else {\n    _typeof = function _typeof(obj) {\n      return obj && typeof Symbol === \"function\" && obj.constructor === Symbol && obj !== Symbol.prototype ? \"symbol\" : typeof obj;\n    };\n  }\n\n  return _typeof(obj);\n}\n\nfunction _toConsumableArray(arr) {\n  if (Array.isArray(arr)) {\n    for (var i = 0, arr2 = new Array(arr.length); i < arr.length; i++) {\n      arr2[i] = arr[i];\n    }\n\n    return arr2;\n  } else {\n    return Array.from(arr);\n  }\n}\n\nfunction hasPlugin(name) {\n  if (name !== \"wast\") throw new Error(\"unknow plugin\");\n  return true;\n}\n\nfunction isKeyword(token, id) {\n  return token.type === _tokenizer.tokens.keyword && token.value === id;\n}\n\nfunction tokenToString(token) {\n  if (token.type === \"keyword\") {\n    return \"keyword (\".concat(token.value, \")\");\n  }\n\n  return token.type;\n}\n\nfunction identifierFromToken(token) {\n  var _token$loc = token.loc,\n      end = _token$loc.end,\n      start = _token$loc.start;\n  return t.withLoc(t.identifier(token.value), end, start);\n}\n\nfunction parse(tokensList, source) {\n  var current = 0;\n  var getUniqueName = t.getUniqueNameGenerator();\n  var state = {\n    registredExportedElements: []\n  }; // But this time we're going to use recursion instead of a `while` loop. So we\n  // define a `walk` function.\n\n  function walk() {\n    var token = tokensList[current];\n\n    function eatToken() {\n      token = tokensList[++current];\n    }\n\n    function getEndLoc() {\n      var currentToken = token;\n\n      if (typeof currentToken === \"undefined\") {\n        var lastToken = tokensList[tokensList.length - 1];\n        currentToken = lastToken;\n      }\n\n      return currentToken.loc.end;\n    }\n\n    function getStartLoc() {\n      return token.loc.start;\n    }\n\n    function eatTokenOfType(type) {\n      if (token.type !== type) {\n        throw new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"Assertion error: expected token of type \" + type + \", given \" + tokenToString(token));\n      }\n\n      eatToken();\n    }\n\n    function parseExportIndex(token) {\n      if (token.type === _tokenizer.tokens.identifier) {\n        var index = identifierFromToken(token);\n        eatToken();\n        return index;\n      } else if (token.type === _tokenizer.tokens.number) {\n        var _index = t.numberLiteralFromRaw(token.value);\n\n        eatToken();\n        return _index;\n      } else {\n        throw function () {\n          return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"unknown export index\" + \", given \" + tokenToString(token));\n        }();\n      }\n    }\n\n    function lookaheadAndCheck() {\n      var len = arguments.length;\n\n      for (var i = 0; i < len; i++) {\n        var tokenAhead = tokensList[current + i];\n        var expectedToken = i < 0 || arguments.length <= i ? undefined : arguments[i];\n\n        if (tokenAhead.type === \"keyword\") {\n          if (isKeyword(tokenAhead, expectedToken) === false) {\n            return false;\n          }\n        } else if (expectedToken !== tokenAhead.type) {\n          return false;\n        }\n      }\n\n      return true;\n    } // TODO(sven): there is probably a better way to do this\n    // can refactor it if it get out of hands\n\n\n    function maybeIgnoreComment() {\n      if (typeof token === \"undefined\") {\n        // Ignore\n        return;\n      }\n\n      while (token.type === _tokenizer.tokens.comment) {\n        eatToken();\n\n        if (typeof token === \"undefined\") {\n          // Hit the end\n          break;\n        }\n      }\n    }\n    /**\n     * Parses a memory instruction\n     *\n     * WAST:\n     *\n     * memory:  ( memory <name>? <memory_sig> )\n     *          ( memory <name>? ( export <string> ) <...> )\n     *          ( memory <name>? ( import <string> <string> ) <memory_sig> )\n     *          ( memory <name>? ( export <string> )* ( data <string>* )\n     * memory_sig: <nat> <nat>?\n     *\n     */\n\n\n    function parseMemory() {\n      var id = t.identifier(getUniqueName(\"memory\"));\n      var limits = t.limit(0);\n\n      if (token.type === _tokenizer.tokens.string || token.type === _tokenizer.tokens.identifier) {\n        id = t.identifier(token.value);\n        eatToken();\n      } else {\n        id = t.withRaw(id, \"\"); // preserve anonymous\n      }\n      /**\n       * Maybe data\n       */\n\n\n      if (lookaheadAndCheck(_tokenizer.tokens.openParen, _tokenizer.keywords.data)) {\n        eatToken(); // (\n\n        eatToken(); // data\n        // TODO(sven): do something with the data collected here\n\n        var stringInitializer = token.value;\n        eatTokenOfType(_tokenizer.tokens.string); // Update limits accordingly\n\n        limits = t.limit(stringInitializer.length);\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n      }\n      /**\n       * Maybe export\n       */\n\n\n      if (lookaheadAndCheck(_tokenizer.tokens.openParen, _tokenizer.keywords.export)) {\n        eatToken(); // (\n\n        eatToken(); // export\n\n        if (token.type !== _tokenizer.tokens.string) {\n          throw function () {\n            return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Expected string in export\" + \", given \" + tokenToString(token));\n          }();\n        }\n\n        var _name = token.value;\n        eatToken();\n        state.registredExportedElements.push({\n          exportType: \"Memory\",\n          name: _name,\n          id: id\n        });\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n      }\n      /**\n       * Memory signature\n       */\n\n\n      if (token.type === _tokenizer.tokens.number) {\n        limits = t.limit((0, _numberLiterals.parse32I)(token.value));\n        eatToken();\n\n        if (token.type === _tokenizer.tokens.number) {\n          limits.max = (0, _numberLiterals.parse32I)(token.value);\n          eatToken();\n        }\n      }\n\n      return t.memory(limits, id);\n    }\n    /**\n     * Parses a data section\n     * https://webassembly.github.io/spec/core/text/modules.html#data-segments\n     *\n     * WAST:\n     *\n     * data:  ( data <index>? <offset> <string> )\n     */\n\n\n    function parseData() {\n      // optional memory index\n      var memidx = 0;\n\n      if (token.type === _tokenizer.tokens.number) {\n        memidx = token.value;\n        eatTokenOfType(_tokenizer.tokens.number); // .\n      }\n\n      eatTokenOfType(_tokenizer.tokens.openParen);\n      var offset;\n\n      if (token.type === _tokenizer.tokens.valtype) {\n        eatTokenOfType(_tokenizer.tokens.valtype); // i32\n\n        eatTokenOfType(_tokenizer.tokens.dot); // .\n\n        if (token.value !== \"const\") {\n          throw new Error(\"constant expression required\");\n        }\n\n        eatTokenOfType(_tokenizer.tokens.name); // const\n\n        var numberLiteral = t.numberLiteralFromRaw(token.value, \"i32\");\n        offset = t.objectInstruction(\"const\", \"i32\", [numberLiteral]);\n        eatToken();\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n      } else {\n        eatTokenOfType(_tokenizer.tokens.name); // get_global\n\n        var _numberLiteral = t.numberLiteralFromRaw(token.value, \"i32\");\n\n        offset = t.instruction(\"get_global\", [_numberLiteral]);\n        eatToken();\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n      }\n\n      var byteArray = (0, _stringLiterals.parseString)(token.value);\n      eatToken(); // \"string\"\n\n      return t.data(t.memIndexLiteral(memidx), offset, t.byteArray(byteArray));\n    }\n    /**\n     * Parses a table instruction\n     *\n     * WAST:\n     *\n     * table:   ( table <name>? <table_type> )\n     *          ( table <name>? ( export <string> ) <...> )\n     *          ( table <name>? ( import <string> <string> ) <table_type> )\n     *          ( table <name>? ( export <string> )* <elem_type> ( elem <var>* ) )\n     *\n     * table_type:  <nat> <nat>? <elem_type>\n     * elem_type: anyfunc\n     *\n     * elem:    ( elem <var>? (offset <instr>* ) <var>* )\n     *          ( elem <var>? <expr> <var>* )\n     */\n\n\n    function parseTable() {\n      var name = t.identifier(getUniqueName(\"table\"));\n      var limit = t.limit(0);\n      var elemIndices = [];\n      var elemType = \"anyfunc\";\n\n      if (token.type === _tokenizer.tokens.string || token.type === _tokenizer.tokens.identifier) {\n        name = identifierFromToken(token);\n        eatToken();\n      } else {\n        name = t.withRaw(name, \"\"); // preserve anonymous\n      }\n\n      while (token.type !== _tokenizer.tokens.closeParen) {\n        /**\n         * Maybe export\n         */\n        if (lookaheadAndCheck(_tokenizer.tokens.openParen, _tokenizer.keywords.elem)) {\n          eatToken(); // (\n\n          eatToken(); // elem\n\n          while (token.type === _tokenizer.tokens.identifier) {\n            elemIndices.push(t.identifier(token.value));\n            eatToken();\n          }\n\n          eatTokenOfType(_tokenizer.tokens.closeParen);\n        } else if (lookaheadAndCheck(_tokenizer.tokens.openParen, _tokenizer.keywords.export)) {\n          eatToken(); // (\n\n          eatToken(); // export\n\n          if (token.type !== _tokenizer.tokens.string) {\n            throw function () {\n              return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Expected string in export\" + \", given \" + tokenToString(token));\n            }();\n          }\n\n          var exportName = token.value;\n          eatToken();\n          state.registredExportedElements.push({\n            exportType: \"Table\",\n            name: exportName,\n            id: name\n          });\n          eatTokenOfType(_tokenizer.tokens.closeParen);\n        } else if (isKeyword(token, _tokenizer.keywords.anyfunc)) {\n          // It's the default value, we can ignore it\n          eatToken(); // anyfunc\n        } else if (token.type === _tokenizer.tokens.number) {\n          /**\n           * Table type\n           */\n          var min = parseInt(token.value);\n          eatToken();\n\n          if (token.type === _tokenizer.tokens.number) {\n            var max = parseInt(token.value);\n            eatToken();\n            limit = t.limit(min, max);\n          } else {\n            limit = t.limit(min);\n          }\n\n          eatToken();\n        } else {\n          throw function () {\n            return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Unexpected token\" + \", given \" + tokenToString(token));\n          }();\n        }\n      }\n\n      if (elemIndices.length > 0) {\n        return t.table(elemType, limit, name, elemIndices);\n      } else {\n        return t.table(elemType, limit, name);\n      }\n    }\n    /**\n     * Parses an import statement\n     *\n     * WAST:\n     *\n     * import:  ( import <string> <string> <imkind> )\n     * imkind:  ( func <name>? <func_sig> )\n     *          ( global <name>? <global_sig> )\n     *          ( table <name>? <table_sig> )\n     *          ( memory <name>? <memory_sig> )\n     *\n     * global_sig: <type> | ( mut <type> )\n     */\n\n\n    function parseImport() {\n      if (token.type !== _tokenizer.tokens.string) {\n        throw new Error(\"Expected a string, \" + token.type + \" given.\");\n      }\n\n      var moduleName = token.value;\n      eatToken();\n\n      if (token.type !== _tokenizer.tokens.string) {\n        throw new Error(\"Expected a string, \" + token.type + \" given.\");\n      }\n\n      var name = token.value;\n      eatToken();\n      eatTokenOfType(_tokenizer.tokens.openParen);\n      var descr;\n\n      if (isKeyword(token, _tokenizer.keywords.func)) {\n        eatToken(); // keyword\n\n        var fnParams = [];\n        var fnResult = [];\n        var typeRef;\n        var fnName = t.identifier(getUniqueName(\"func\"));\n\n        if (token.type === _tokenizer.tokens.identifier) {\n          fnName = identifierFromToken(token);\n          eatToken();\n        }\n\n        while (token.type === _tokenizer.tokens.openParen) {\n          eatToken();\n\n          if (lookaheadAndCheck(_tokenizer.keywords.type) === true) {\n            eatToken();\n            typeRef = parseTypeReference();\n          } else if (lookaheadAndCheck(_tokenizer.keywords.param) === true) {\n            eatToken();\n            fnParams.push.apply(fnParams, _toConsumableArray(parseFuncParam()));\n          } else if (lookaheadAndCheck(_tokenizer.keywords.result) === true) {\n            eatToken();\n            fnResult.push.apply(fnResult, _toConsumableArray(parseFuncResult()));\n          } else {\n            throw function () {\n              return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Unexpected token in import of type\" + \", given \" + tokenToString(token));\n            }();\n          }\n\n          eatTokenOfType(_tokenizer.tokens.closeParen);\n        }\n\n        if (typeof fnName === \"undefined\") {\n          throw new Error(\"Imported function must have a name\");\n        }\n\n        descr = t.funcImportDescr(fnName, typeRef !== undefined ? typeRef : t.signature(fnParams, fnResult));\n      } else if (isKeyword(token, _tokenizer.keywords.global)) {\n        eatToken(); // keyword\n\n        if (token.type === _tokenizer.tokens.openParen) {\n          eatToken(); // (\n\n          eatTokenOfType(_tokenizer.tokens.keyword); // mut keyword\n\n          var valtype = token.value;\n          eatToken();\n          descr = t.globalType(valtype, \"var\");\n          eatTokenOfType(_tokenizer.tokens.closeParen);\n        } else {\n          var _valtype = token.value;\n          eatTokenOfType(_tokenizer.tokens.valtype);\n          descr = t.globalType(_valtype, \"const\");\n        }\n      } else if (isKeyword(token, _tokenizer.keywords.memory) === true) {\n        eatToken(); // Keyword\n\n        descr = parseMemory();\n      } else if (isKeyword(token, _tokenizer.keywords.table) === true) {\n        eatToken(); // Keyword\n\n        descr = parseTable();\n      } else {\n        throw new Error(\"Unsupported import type: \" + tokenToString(token));\n      }\n\n      eatTokenOfType(_tokenizer.tokens.closeParen);\n      return t.moduleImport(moduleName, name, descr);\n    }\n    /**\n     * Parses a block instruction\n     *\n     * WAST:\n     *\n     * expr: ( block <name>? <block_sig> <instr>* )\n     * instr: block <name>? <block_sig> <instr>* end <name>?\n     * block_sig : ( result <type>* )*\n     *\n     */\n\n\n    function parseBlock() {\n      var label = t.identifier(getUniqueName(\"block\"));\n      var blockResult = null;\n      var instr = [];\n\n      if (token.type === _tokenizer.tokens.identifier) {\n        label = identifierFromToken(token);\n        eatToken();\n      } else {\n        label = t.withRaw(label, \"\"); // preserve anonymous\n      }\n\n      while (token.type === _tokenizer.tokens.openParen) {\n        eatToken();\n\n        if (lookaheadAndCheck(_tokenizer.keywords.result) === true) {\n          eatToken();\n          blockResult = token.value;\n          eatToken();\n        } else if (lookaheadAndCheck(_tokenizer.tokens.name) === true || lookaheadAndCheck(_tokenizer.tokens.valtype) === true || token.type === \"keyword\" // is any keyword\n        ) {\n            // Instruction\n            instr.push(parseFuncInstr());\n          } else {\n          throw function () {\n            return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Unexpected token in block body of type\" + \", given \" + tokenToString(token));\n          }();\n        }\n\n        maybeIgnoreComment();\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n      }\n\n      return t.blockInstruction(label, instr, blockResult);\n    }\n    /**\n     * Parses a if instruction\n     *\n     * WAST:\n     *\n     * expr:\n     * ( if <name>? <block_sig> ( then <instr>* ) ( else <instr>* )? )\n     * ( if <name>? <block_sig> <expr>+ ( then <instr>* ) ( else <instr>* )? )\n     *\n     * instr:\n     * if <name>? <block_sig> <instr>* end <name>?\n     * if <name>? <block_sig> <instr>* else <name>? <instr>* end <name>?\n     *\n     * block_sig : ( result <type>* )*\n     *\n     */\n\n\n    function parseIf() {\n      var blockResult = null;\n      var label = t.identifier(getUniqueName(\"if\"));\n      var testInstrs = [];\n      var consequent = [];\n      var alternate = [];\n\n      if (token.type === _tokenizer.tokens.identifier) {\n        label = identifierFromToken(token);\n        eatToken();\n      } else {\n        label = t.withRaw(label, \"\"); // preserve anonymous\n      }\n\n      while (token.type === _tokenizer.tokens.openParen) {\n        eatToken(); // (\n\n        /**\n         * Block signature\n         */\n\n        if (isKeyword(token, _tokenizer.keywords.result) === true) {\n          eatToken();\n          blockResult = token.value;\n          eatTokenOfType(_tokenizer.tokens.valtype);\n          eatTokenOfType(_tokenizer.tokens.closeParen);\n          continue;\n        }\n        /**\n         * Then\n         */\n\n\n        if (isKeyword(token, _tokenizer.keywords.then) === true) {\n          eatToken(); // then\n\n          while (token.type === _tokenizer.tokens.openParen) {\n            eatToken(); // Instruction\n\n            if (lookaheadAndCheck(_tokenizer.tokens.name) === true || lookaheadAndCheck(_tokenizer.tokens.valtype) === true || token.type === \"keyword\" // is any keyword\n            ) {\n                consequent.push(parseFuncInstr());\n              } else {\n              throw function () {\n                return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Unexpected token in consequent body of type\" + \", given \" + tokenToString(token));\n              }();\n            }\n\n            eatTokenOfType(_tokenizer.tokens.closeParen);\n          }\n\n          eatTokenOfType(_tokenizer.tokens.closeParen);\n          continue;\n        }\n        /**\n         * Alternate\n         */\n\n\n        if (isKeyword(token, _tokenizer.keywords.else)) {\n          eatToken(); // else\n\n          while (token.type === _tokenizer.tokens.openParen) {\n            eatToken(); // Instruction\n\n            if (lookaheadAndCheck(_tokenizer.tokens.name) === true || lookaheadAndCheck(_tokenizer.tokens.valtype) === true || token.type === \"keyword\" // is any keyword\n            ) {\n                alternate.push(parseFuncInstr());\n              } else {\n              throw function () {\n                return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Unexpected token in alternate body of type\" + \", given \" + tokenToString(token));\n              }();\n            }\n\n            eatTokenOfType(_tokenizer.tokens.closeParen);\n          }\n\n          eatTokenOfType(_tokenizer.tokens.closeParen);\n          continue;\n        }\n        /**\n         * Test instruction\n         */\n\n\n        if (lookaheadAndCheck(_tokenizer.tokens.name) === true || lookaheadAndCheck(_tokenizer.tokens.valtype) === true || token.type === \"keyword\" // is any keyword\n        ) {\n            testInstrs.push(parseFuncInstr());\n            eatTokenOfType(_tokenizer.tokens.closeParen);\n            continue;\n          }\n\n        throw function () {\n          return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Unexpected token in if body\" + \", given \" + tokenToString(token));\n        }();\n      }\n\n      return t.ifInstruction(label, testInstrs, blockResult, consequent, alternate);\n    }\n    /**\n     * Parses a loop instruction\n     *\n     * WAT:\n     *\n     * blockinstr :: 'loop' I:label rt:resulttype (in:instr*) 'end' id?\n     *\n     * WAST:\n     *\n     * instr     :: loop <name>? <block_sig> <instr>* end <name>?\n     * expr      :: ( loop <name>? <block_sig> <instr>* )\n     * block_sig :: ( result <type>* )*\n     *\n     */\n\n\n    function parseLoop() {\n      var label = t.identifier(getUniqueName(\"loop\"));\n      var blockResult;\n      var instr = [];\n\n      if (token.type === _tokenizer.tokens.identifier) {\n        label = identifierFromToken(token);\n        eatToken();\n      } else {\n        label = t.withRaw(label, \"\"); // preserve anonymous\n      }\n\n      while (token.type === _tokenizer.tokens.openParen) {\n        eatToken();\n\n        if (lookaheadAndCheck(_tokenizer.keywords.result) === true) {\n          eatToken();\n          blockResult = token.value;\n          eatToken();\n        } else if (lookaheadAndCheck(_tokenizer.tokens.name) === true || lookaheadAndCheck(_tokenizer.tokens.valtype) === true || token.type === \"keyword\" // is any keyword\n        ) {\n            // Instruction\n            instr.push(parseFuncInstr());\n          } else {\n          throw function () {\n            return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Unexpected token in loop body\" + \", given \" + tokenToString(token));\n          }();\n        }\n\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n      }\n\n      return t.loopInstruction(label, blockResult, instr);\n    }\n\n    function parseCallIndirect() {\n      var typeRef;\n      var params = [];\n      var results = [];\n      var instrs = [];\n\n      while (token.type !== _tokenizer.tokens.closeParen) {\n        if (lookaheadAndCheck(_tokenizer.tokens.openParen, _tokenizer.keywords.type)) {\n          eatToken(); // (\n\n          eatToken(); // type\n\n          typeRef = parseTypeReference();\n        } else if (lookaheadAndCheck(_tokenizer.tokens.openParen, _tokenizer.keywords.param)) {\n          eatToken(); // (\n\n          eatToken(); // param\n\n          /**\n           * Params can be empty:\n           * (params)`\n           */\n\n          if (token.type !== _tokenizer.tokens.closeParen) {\n            params.push.apply(params, _toConsumableArray(parseFuncParam()));\n          }\n        } else if (lookaheadAndCheck(_tokenizer.tokens.openParen, _tokenizer.keywords.result)) {\n          eatToken(); // (\n\n          eatToken(); // result\n\n          /**\n           * Results can be empty:\n           * (result)`\n           */\n\n          if (token.type !== _tokenizer.tokens.closeParen) {\n            results.push.apply(results, _toConsumableArray(parseFuncResult()));\n          }\n        } else {\n          eatTokenOfType(_tokenizer.tokens.openParen);\n          instrs.push(parseFuncInstr());\n        }\n\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n      }\n\n      return t.callIndirectInstruction(typeRef !== undefined ? typeRef : t.signature(params, results), instrs);\n    }\n    /**\n     * Parses an export instruction\n     *\n     * WAT:\n     *\n     * export:  ( export <string> <exkind> )\n     * exkind:  ( func <var> )\n     *          ( global <var> )\n     *          ( table <var> )\n     *          ( memory <var> )\n     * var:    <nat> | <name>\n     *\n     */\n\n\n    function parseExport() {\n      if (token.type !== _tokenizer.tokens.string) {\n        throw new Error(\"Expected string after export, got: \" + token.type);\n      }\n\n      var name = token.value;\n      eatToken();\n      var moduleExportDescr = parseModuleExportDescr();\n      return t.moduleExport(name, moduleExportDescr);\n    }\n\n    function parseModuleExportDescr() {\n      var startLoc = getStartLoc();\n      var type = \"\";\n      var index;\n      eatTokenOfType(_tokenizer.tokens.openParen);\n\n      while (token.type !== _tokenizer.tokens.closeParen) {\n        if (isKeyword(token, _tokenizer.keywords.func)) {\n          type = \"Func\";\n          eatToken();\n          index = parseExportIndex(token);\n        } else if (isKeyword(token, _tokenizer.keywords.table)) {\n          type = \"Table\";\n          eatToken();\n          index = parseExportIndex(token);\n        } else if (isKeyword(token, _tokenizer.keywords.global)) {\n          type = \"Global\";\n          eatToken();\n          index = parseExportIndex(token);\n        } else if (isKeyword(token, _tokenizer.keywords.memory)) {\n          type = \"Memory\";\n          eatToken();\n          index = parseExportIndex(token);\n        }\n\n        eatToken();\n      }\n\n      if (type === \"\") {\n        throw new Error(\"Unknown export type\");\n      }\n\n      if (index === undefined) {\n        throw new Error(\"Exported function must have a name\");\n      }\n\n      var node = t.moduleExportDescr(type, index);\n      var endLoc = getEndLoc();\n      eatTokenOfType(_tokenizer.tokens.closeParen);\n      return t.withLoc(node, endLoc, startLoc);\n    }\n\n    function parseModule() {\n      var name = null;\n      var isBinary = false;\n      var isQuote = false;\n      var moduleFields = [];\n\n      if (token.type === _tokenizer.tokens.identifier) {\n        name = token.value;\n        eatToken();\n      }\n\n      if (hasPlugin(\"wast\") && token.type === _tokenizer.tokens.name && token.value === \"binary\") {\n        eatToken();\n        isBinary = true;\n      }\n\n      if (hasPlugin(\"wast\") && token.type === _tokenizer.tokens.name && token.value === \"quote\") {\n        eatToken();\n        isQuote = true;\n      }\n\n      if (isBinary === true) {\n        var blob = [];\n\n        while (token.type === _tokenizer.tokens.string) {\n          blob.push(token.value);\n          eatToken();\n          maybeIgnoreComment();\n        }\n\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n        return t.binaryModule(name, blob);\n      }\n\n      if (isQuote === true) {\n        var string = [];\n\n        while (token.type === _tokenizer.tokens.string) {\n          string.push(token.value);\n          eatToken();\n        }\n\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n        return t.quoteModule(name, string);\n      }\n\n      while (token.type !== _tokenizer.tokens.closeParen) {\n        moduleFields.push(walk());\n\n        if (state.registredExportedElements.length > 0) {\n          state.registredExportedElements.forEach(function (decl) {\n            moduleFields.push(t.moduleExport(decl.name, t.moduleExportDescr(decl.exportType, decl.id)));\n          });\n          state.registredExportedElements = [];\n        }\n\n        token = tokensList[current];\n      }\n\n      eatTokenOfType(_tokenizer.tokens.closeParen);\n      return t.module(name, moduleFields);\n    }\n    /**\n     * Parses the arguments of an instruction\n     */\n\n\n    function parseFuncInstrArguments(signature) {\n      var args = [];\n      var namedArgs = {};\n      var signaturePtr = 0;\n\n      while (token.type === _tokenizer.tokens.name || isKeyword(token, _tokenizer.keywords.offset)) {\n        var key = token.value;\n        eatToken();\n        eatTokenOfType(_tokenizer.tokens.equal);\n        var value = void 0;\n\n        if (token.type === _tokenizer.tokens.number) {\n          value = t.numberLiteralFromRaw(token.value);\n        } else {\n          throw new Error(\"Unexpected type for argument: \" + token.type);\n        }\n\n        namedArgs[key] = value;\n        eatToken();\n      } // $FlowIgnore\n\n\n      var signatureLength = signature.vector ? Infinity : signature.length;\n\n      while (token.type !== _tokenizer.tokens.closeParen && ( // $FlowIgnore\n      token.type === _tokenizer.tokens.openParen || signaturePtr < signatureLength)) {\n        if (token.type === _tokenizer.tokens.identifier) {\n          args.push(t.identifier(token.value));\n          eatToken();\n        } else if (token.type === _tokenizer.tokens.valtype) {\n          // Handle locals\n          args.push(t.valtypeLiteral(token.value));\n          eatToken();\n        } else if (token.type === _tokenizer.tokens.string) {\n          args.push(t.stringLiteral(token.value));\n          eatToken();\n        } else if (token.type === _tokenizer.tokens.number) {\n          args.push( // TODO(sven): refactor the type signature handling\n          // https://github.com/xtuc/webassemblyjs/pull/129 is a good start\n          t.numberLiteralFromRaw(token.value, // $FlowIgnore\n          signature[signaturePtr] || \"f64\")); // $FlowIgnore\n\n          if (!signature.vector) {\n            ++signaturePtr;\n          }\n\n          eatToken();\n        } else if (token.type === _tokenizer.tokens.openParen) {\n          /**\n           * Maybe some nested instructions\n           */\n          eatToken(); // Instruction\n\n          if (lookaheadAndCheck(_tokenizer.tokens.name) === true || lookaheadAndCheck(_tokenizer.tokens.valtype) === true || token.type === \"keyword\" // is any keyword\n          ) {\n              // $FlowIgnore\n              args.push(parseFuncInstr());\n            } else {\n            throw function () {\n              return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Unexpected token in nested instruction\" + \", given \" + tokenToString(token));\n            }();\n          }\n\n          if (token.type === _tokenizer.tokens.closeParen) {\n            eatToken();\n          }\n        } else {\n          throw function () {\n            return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Unexpected token in instruction argument\" + \", given \" + tokenToString(token));\n          }();\n        }\n      }\n\n      return {\n        args: args,\n        namedArgs: namedArgs\n      };\n    }\n    /**\n     * Parses an instruction\n     *\n     * WAT:\n     *\n     * instr      :: plaininst\n     *               blockinstr\n     *\n     * blockinstr :: 'block' I:label rt:resulttype (in:instr*) 'end' id?\n     *               'loop' I:label rt:resulttype (in:instr*) 'end' id?\n     *               'if' I:label rt:resulttype (in:instr*) 'else' id? (in2:intr*) 'end' id?\n     *\n     * plaininst  :: 'unreachable'\n     *               'nop'\n     *               'br' l:labelidx\n     *               'br_if' l:labelidx\n     *               'br_table' l*:vec(labelidx) ln:labelidx\n     *               'return'\n     *               'call' x:funcidx\n     *               'call_indirect' x, I:typeuse\n     *\n     * WAST:\n     *\n     * instr:\n     *   <expr>\n     *   <op>\n     *   block <name>? <block_sig> <instr>* end <name>?\n     *   loop <name>? <block_sig> <instr>* end <name>?\n     *   if <name>? <block_sig> <instr>* end <name>?\n     *   if <name>? <block_sig> <instr>* else <name>? <instr>* end <name>?\n     *\n     * expr:\n     *   ( <op> )\n     *   ( <op> <expr>+ )\n     *   ( block <name>? <block_sig> <instr>* )\n     *   ( loop <name>? <block_sig> <instr>* )\n     *   ( if <name>? <block_sig> ( then <instr>* ) ( else <instr>* )? )\n     *   ( if <name>? <block_sig> <expr>+ ( then <instr>* ) ( else <instr>* )? )\n     *\n     * op:\n     *   unreachable\n     *   nop\n     *   br <var>\n     *   br_if <var>\n     *   br_table <var>+\n     *   return\n     *   call <var>\n     *   call_indirect <func_sig>\n     *   drop\n     *   select\n     *   get_local <var>\n     *   set_local <var>\n     *   tee_local <var>\n     *   get_global <var>\n     *   set_global <var>\n     *   <type>.load((8|16|32)_<sign>)? <offset>? <align>?\n     *   <type>.store(8|16|32)? <offset>? <align>?\n     *   current_memory\n     *   grow_memory\n     *   <type>.const <value>\n     *   <type>.<unop>\n     *   <type>.<binop>\n     *   <type>.<testop>\n     *   <type>.<relop>\n     *   <type>.<cvtop>/<type>\n     *\n     * func_type:   ( type <var> )? <param>* <result>*\n     */\n\n\n    function parseFuncInstr() {\n      var startLoc = getStartLoc();\n      maybeIgnoreComment();\n      /**\n       * A simple instruction\n       */\n\n      if (token.type === _tokenizer.tokens.name || token.type === _tokenizer.tokens.valtype) {\n        var _name2 = token.value;\n        var object;\n        eatToken();\n\n        if (token.type === _tokenizer.tokens.dot) {\n          object = _name2;\n          eatToken();\n\n          if (token.type !== _tokenizer.tokens.name) {\n            throw new TypeError(\"Unknown token: \" + token.type + \", name expected\");\n          }\n\n          _name2 = token.value;\n          eatToken();\n        }\n\n        if (token.type === _tokenizer.tokens.closeParen) {\n          var _endLoc = token.loc.end;\n\n          if (typeof object === \"undefined\") {\n            return t.withLoc(t.instruction(_name2), _endLoc, startLoc);\n          } else {\n            return t.withLoc(t.objectInstruction(_name2, object, []), _endLoc, startLoc);\n          }\n        }\n\n        var signature = t.signatureForOpcode(object || \"\", _name2);\n\n        var _parseFuncInstrArgume = parseFuncInstrArguments(signature),\n            _args = _parseFuncInstrArgume.args,\n            _namedArgs = _parseFuncInstrArgume.namedArgs;\n\n        var endLoc = token.loc.end;\n\n        if (typeof object === \"undefined\") {\n          return t.withLoc(t.instruction(_name2, _args, _namedArgs), endLoc, startLoc);\n        } else {\n          return t.withLoc(t.objectInstruction(_name2, object, _args, _namedArgs), endLoc, startLoc);\n        }\n      } else if (isKeyword(token, _tokenizer.keywords.loop)) {\n        /**\n         * Else a instruction with a keyword (loop or block)\n         */\n        eatToken(); // keyword\n\n        return parseLoop();\n      } else if (isKeyword(token, _tokenizer.keywords.block)) {\n        eatToken(); // keyword\n\n        return parseBlock();\n      } else if (isKeyword(token, _tokenizer.keywords.call_indirect)) {\n        eatToken(); // keyword\n\n        return parseCallIndirect();\n      } else if (isKeyword(token, _tokenizer.keywords.call)) {\n        eatToken(); // keyword\n\n        var index;\n\n        if (token.type === _tokenizer.tokens.identifier) {\n          index = identifierFromToken(token);\n          eatToken();\n        } else if (token.type === _tokenizer.tokens.number) {\n          index = t.indexLiteral(token.value);\n          eatToken();\n        }\n\n        var instrArgs = []; // Nested instruction\n\n        while (token.type === _tokenizer.tokens.openParen) {\n          eatToken();\n          instrArgs.push(parseFuncInstr());\n          eatTokenOfType(_tokenizer.tokens.closeParen);\n        }\n\n        if (typeof index === \"undefined\") {\n          throw new Error(\"Missing argument in call instruciton\");\n        }\n\n        if (instrArgs.length > 0) {\n          return t.callInstruction(index, instrArgs);\n        } else {\n          return t.callInstruction(index);\n        }\n      } else if (isKeyword(token, _tokenizer.keywords.if)) {\n        eatToken(); // Keyword\n\n        return parseIf();\n      } else if (isKeyword(token, _tokenizer.keywords.module) && hasPlugin(\"wast\")) {\n        eatToken(); // In WAST you can have a module as an instruction's argument\n        // we will cast it into a instruction to not break the flow\n        // $FlowIgnore\n\n        var module = parseModule();\n        return module;\n      } else {\n        throw function () {\n          return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Unexpected instruction in function body\" + \", given \" + tokenToString(token));\n        }();\n      }\n    }\n    /*\n     * Parses a function\n     *\n     * WAT:\n     *\n     * functype :: ( 'func' t1:vec(param) t2:vec(result) )\n     * param    :: ( 'param' id? t:valtype )\n     * result   :: ( 'result' t:valtype )\n     *\n     * WAST:\n     *\n     * func     :: ( func <name>? <func_sig> <local>* <instr>* )\n     *             ( func <name>? ( export <string> ) <...> )\n     *             ( func <name>? ( import <string> <string> ) <func_sig> )\n     * func_sig :: ( type <var> )? <param>* <result>*\n     * param    :: ( param <type>* ) | ( param <name> <type> )\n     * result   :: ( result <type>* )\n     * local    :: ( local <type>* ) | ( local <name> <type> )\n     *\n     */\n\n\n    function parseFunc() {\n      var fnName = t.identifier(getUniqueName(\"func\"));\n      var typeRef;\n      var fnBody = [];\n      var fnParams = [];\n      var fnResult = []; // name\n\n      if (token.type === _tokenizer.tokens.identifier) {\n        fnName = identifierFromToken(token);\n        eatToken();\n      } else {\n        fnName = t.withRaw(fnName, \"\"); // preserve anonymous\n      }\n\n      maybeIgnoreComment();\n\n      while (token.type === _tokenizer.tokens.openParen || token.type === _tokenizer.tokens.name || token.type === _tokenizer.tokens.valtype) {\n        // Instructions without parens\n        if (token.type === _tokenizer.tokens.name || token.type === _tokenizer.tokens.valtype) {\n          fnBody.push(parseFuncInstr());\n          continue;\n        }\n\n        eatToken();\n\n        if (lookaheadAndCheck(_tokenizer.keywords.param) === true) {\n          eatToken();\n          fnParams.push.apply(fnParams, _toConsumableArray(parseFuncParam()));\n        } else if (lookaheadAndCheck(_tokenizer.keywords.result) === true) {\n          eatToken();\n          fnResult.push.apply(fnResult, _toConsumableArray(parseFuncResult()));\n        } else if (lookaheadAndCheck(_tokenizer.keywords.export) === true) {\n          eatToken();\n          parseFuncExport(fnName);\n        } else if (lookaheadAndCheck(_tokenizer.keywords.type) === true) {\n          eatToken();\n          typeRef = parseTypeReference();\n        } else if (lookaheadAndCheck(_tokenizer.tokens.name) === true || lookaheadAndCheck(_tokenizer.tokens.valtype) === true || token.type === \"keyword\" // is any keyword\n        ) {\n            // Instruction\n            fnBody.push(parseFuncInstr());\n          } else {\n          throw function () {\n            return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Unexpected token in func body\" + \", given \" + tokenToString(token));\n          }();\n        }\n\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n      }\n\n      return t.func(fnName, typeRef !== undefined ? typeRef : t.signature(fnParams, fnResult), fnBody);\n    }\n    /**\n     * Parses shorthand export in func\n     *\n     * export :: ( export <string> )\n     */\n\n\n    function parseFuncExport(funcId) {\n      if (token.type !== _tokenizer.tokens.string) {\n        throw function () {\n          return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Function export expected a string\" + \", given \" + tokenToString(token));\n        }();\n      }\n\n      var name = token.value;\n      eatToken();\n      /**\n       * Func export shorthand, we trait it as a syntaxic sugar.\n       * A export ModuleField will be added later.\n       *\n       * We give the anonymous function a generated name and export it.\n       */\n\n      var id = t.identifier(funcId.value);\n      state.registredExportedElements.push({\n        exportType: \"Func\",\n        name: name,\n        id: id\n      });\n    }\n    /**\n     * Parses a type instruction\n     *\n     * WAST:\n     *\n     * typedef: ( type <name>? ( func <param>* <result>* ) )\n     */\n\n\n    function parseType() {\n      var id;\n      var params = [];\n      var result = [];\n\n      if (token.type === _tokenizer.tokens.identifier) {\n        id = identifierFromToken(token);\n        eatToken();\n      }\n\n      if (lookaheadAndCheck(_tokenizer.tokens.openParen, _tokenizer.keywords.func)) {\n        eatToken(); // (\n\n        eatToken(); // func\n\n        if (token.type === _tokenizer.tokens.closeParen) {\n          eatToken(); // function with an empty signature, we can abort here\n\n          return t.typeInstruction(id, t.signature([], []));\n        }\n\n        if (lookaheadAndCheck(_tokenizer.tokens.openParen, _tokenizer.keywords.param)) {\n          eatToken(); // (\n\n          eatToken(); // param\n\n          params = parseFuncParam();\n          eatTokenOfType(_tokenizer.tokens.closeParen);\n        }\n\n        if (lookaheadAndCheck(_tokenizer.tokens.openParen, _tokenizer.keywords.result)) {\n          eatToken(); // (\n\n          eatToken(); // result\n\n          result = parseFuncResult();\n          eatTokenOfType(_tokenizer.tokens.closeParen);\n        }\n\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n      }\n\n      return t.typeInstruction(id, t.signature(params, result));\n    }\n    /**\n     * Parses a function result\n     *\n     * WAST:\n     *\n     * result :: ( result <type>* )\n     */\n\n\n    function parseFuncResult() {\n      var results = [];\n\n      while (token.type !== _tokenizer.tokens.closeParen) {\n        if (token.type !== _tokenizer.tokens.valtype) {\n          throw function () {\n            return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Unexpected token in func result\" + \", given \" + tokenToString(token));\n          }();\n        }\n\n        var valtype = token.value;\n        eatToken();\n        results.push(valtype);\n      }\n\n      return results;\n    }\n    /**\n     * Parses a type reference\n     *\n     */\n\n\n    function parseTypeReference() {\n      var ref;\n\n      if (token.type === _tokenizer.tokens.identifier) {\n        ref = identifierFromToken(token);\n        eatToken();\n      } else if (token.type === _tokenizer.tokens.number) {\n        ref = t.numberLiteralFromRaw(token.value);\n        eatToken();\n      }\n\n      return ref;\n    }\n    /**\n     * Parses a global instruction\n     *\n     * WAST:\n     *\n     * global:  ( global <name>? <global_sig> <instr>* )\n     *          ( global <name>? ( export <string> ) <...> )\n     *          ( global <name>? ( import <string> <string> ) <global_sig> )\n     *\n     * global_sig: <type> | ( mut <type> )\n     *\n     */\n\n\n    function parseGlobal() {\n      var name = t.identifier(getUniqueName(\"global\"));\n      var type; // Keep informations in case of a shorthand import\n\n      var importing = null;\n      maybeIgnoreComment();\n\n      if (token.type === _tokenizer.tokens.identifier) {\n        name = identifierFromToken(token);\n        eatToken();\n      } else {\n        name = t.withRaw(name, \"\"); // preserve anonymous\n      }\n      /**\n       * maybe export\n       */\n\n\n      if (lookaheadAndCheck(_tokenizer.tokens.openParen, _tokenizer.keywords.export)) {\n        eatToken(); // (\n\n        eatToken(); // export\n\n        var exportName = token.value;\n        eatTokenOfType(_tokenizer.tokens.string);\n        state.registredExportedElements.push({\n          exportType: \"Global\",\n          name: exportName,\n          id: name\n        });\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n      }\n      /**\n       * maybe import\n       */\n\n\n      if (lookaheadAndCheck(_tokenizer.tokens.openParen, _tokenizer.keywords.import)) {\n        eatToken(); // (\n\n        eatToken(); // import\n\n        var moduleName = token.value;\n        eatTokenOfType(_tokenizer.tokens.string);\n        var _name3 = token.value;\n        eatTokenOfType(_tokenizer.tokens.string);\n        importing = {\n          module: moduleName,\n          name: _name3,\n          descr: undefined\n        };\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n      }\n      /**\n       * global_sig\n       */\n\n\n      if (token.type === _tokenizer.tokens.valtype) {\n        type = t.globalType(token.value, \"const\");\n        eatToken();\n      } else if (token.type === _tokenizer.tokens.openParen) {\n        eatToken(); // (\n\n        if (isKeyword(token, _tokenizer.keywords.mut) === false) {\n          throw function () {\n            return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Unsupported global type, expected mut\" + \", given \" + tokenToString(token));\n          }();\n        }\n\n        eatToken(); // mut\n\n        type = t.globalType(token.value, \"var\");\n        eatToken();\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n      }\n\n      if (type === undefined) {\n        throw function () {\n          return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Could not determine global type\" + \", given \" + tokenToString(token));\n        }();\n      }\n\n      maybeIgnoreComment();\n      var init = [];\n\n      if (importing != null) {\n        importing.descr = type;\n        init.push(t.moduleImport(importing.module, importing.name, importing.descr));\n      }\n      /**\n       * instr*\n       */\n\n\n      while (token.type === _tokenizer.tokens.openParen) {\n        eatToken();\n        init.push(parseFuncInstr());\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n      }\n\n      return t.global(type, init, name);\n    }\n    /**\n     * Parses a function param\n     *\n     * WAST:\n     *\n     * param    :: ( param <type>* ) | ( param <name> <type> )\n     */\n\n\n    function parseFuncParam() {\n      var params = [];\n      var id;\n      var valtype;\n\n      if (token.type === _tokenizer.tokens.identifier) {\n        id = token.value;\n        eatToken();\n      }\n\n      if (token.type === _tokenizer.tokens.valtype) {\n        valtype = token.value;\n        eatToken();\n        params.push({\n          id: id,\n          valtype: valtype\n        });\n        /**\n         * Shorthand notation for multiple anonymous parameters\n         * @see https://webassembly.github.io/spec/core/text/types.html#function-types\n         * @see https://github.com/xtuc/webassemblyjs/issues/6\n         */\n\n        if (id === undefined) {\n          while (token.type === _tokenizer.tokens.valtype) {\n            valtype = token.value;\n            eatToken();\n            params.push({\n              id: undefined,\n              valtype: valtype\n            });\n          }\n        }\n      } else {// ignore\n      }\n\n      return params;\n    }\n    /**\n     * Parses an element segments instruction\n     *\n     * WAST:\n     *\n     * elem:    ( elem <var>? (offset <instr>* ) <var>* )\n     *          ( elem <var>? <expr> <var>* )\n     *\n     * var:    <nat> | <name>\n     */\n\n\n    function parseElem() {\n      var tableIndex = t.indexLiteral(0);\n      var offset = [];\n      var funcs = [];\n\n      if (token.type === _tokenizer.tokens.identifier) {\n        tableIndex = identifierFromToken(token);\n        eatToken();\n      }\n\n      if (token.type === _tokenizer.tokens.number) {\n        tableIndex = t.indexLiteral(token.value);\n        eatToken();\n      }\n\n      while (token.type !== _tokenizer.tokens.closeParen) {\n        if (lookaheadAndCheck(_tokenizer.tokens.openParen, _tokenizer.keywords.offset)) {\n          eatToken(); // (\n\n          eatToken(); // offset\n\n          while (token.type !== _tokenizer.tokens.closeParen) {\n            eatTokenOfType(_tokenizer.tokens.openParen);\n            offset.push(parseFuncInstr());\n            eatTokenOfType(_tokenizer.tokens.closeParen);\n          }\n\n          eatTokenOfType(_tokenizer.tokens.closeParen);\n        } else if (token.type === _tokenizer.tokens.identifier) {\n          funcs.push(t.identifier(token.value));\n          eatToken();\n        } else if (token.type === _tokenizer.tokens.number) {\n          funcs.push(t.indexLiteral(token.value));\n          eatToken();\n        } else if (token.type === _tokenizer.tokens.openParen) {\n          eatToken(); // (\n\n          offset.push(parseFuncInstr());\n          eatTokenOfType(_tokenizer.tokens.closeParen);\n        } else {\n          throw function () {\n            return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Unsupported token in elem\" + \", given \" + tokenToString(token));\n          }();\n        }\n      }\n\n      return t.elem(tableIndex, offset, funcs);\n    }\n    /**\n     * Parses the start instruction in a module\n     *\n     * WAST:\n     *\n     * start:   ( start <var> )\n     * var:    <nat> | <name>\n     *\n     * WAT:\n     * start ::= ( start  x:funcidx )\n     */\n\n\n    function parseStart() {\n      if (token.type === _tokenizer.tokens.identifier) {\n        var index = identifierFromToken(token);\n        eatToken();\n        return t.start(index);\n      }\n\n      if (token.type === _tokenizer.tokens.number) {\n        var _index2 = t.indexLiteral(token.value);\n\n        eatToken();\n        return t.start(_index2);\n      }\n\n      throw new Error(\"Unknown start, token: \" + tokenToString(token));\n    }\n\n    if (token.type === _tokenizer.tokens.openParen) {\n      eatToken();\n      var startLoc = getStartLoc();\n\n      if (isKeyword(token, _tokenizer.keywords.export)) {\n        eatToken();\n        var node = parseExport();\n\n        var _endLoc2 = getEndLoc();\n\n        return t.withLoc(node, _endLoc2, startLoc);\n      }\n\n      if (isKeyword(token, _tokenizer.keywords.loop)) {\n        eatToken();\n\n        var _node = parseLoop();\n\n        var _endLoc3 = getEndLoc();\n\n        return t.withLoc(_node, _endLoc3, startLoc);\n      }\n\n      if (isKeyword(token, _tokenizer.keywords.func)) {\n        eatToken();\n\n        var _node2 = parseFunc();\n\n        var _endLoc4 = getEndLoc();\n\n        maybeIgnoreComment();\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n        return t.withLoc(_node2, _endLoc4, startLoc);\n      }\n\n      if (isKeyword(token, _tokenizer.keywords.module)) {\n        eatToken();\n\n        var _node3 = parseModule();\n\n        var _endLoc5 = getEndLoc();\n\n        return t.withLoc(_node3, _endLoc5, startLoc);\n      }\n\n      if (isKeyword(token, _tokenizer.keywords.import)) {\n        eatToken();\n\n        var _node4 = parseImport();\n\n        var _endLoc6 = getEndLoc();\n\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n        return t.withLoc(_node4, _endLoc6, startLoc);\n      }\n\n      if (isKeyword(token, _tokenizer.keywords.block)) {\n        eatToken();\n\n        var _node5 = parseBlock();\n\n        var _endLoc7 = getEndLoc();\n\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n        return t.withLoc(_node5, _endLoc7, startLoc);\n      }\n\n      if (isKeyword(token, _tokenizer.keywords.memory)) {\n        eatToken();\n\n        var _node6 = parseMemory();\n\n        var _endLoc8 = getEndLoc();\n\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n        return t.withLoc(_node6, _endLoc8, startLoc);\n      }\n\n      if (isKeyword(token, _tokenizer.keywords.data)) {\n        eatToken();\n\n        var _node7 = parseData();\n\n        var _endLoc9 = getEndLoc();\n\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n        return t.withLoc(_node7, _endLoc9, startLoc);\n      }\n\n      if (isKeyword(token, _tokenizer.keywords.table)) {\n        eatToken();\n\n        var _node8 = parseTable();\n\n        var _endLoc10 = getEndLoc();\n\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n        return t.withLoc(_node8, _endLoc10, startLoc);\n      }\n\n      if (isKeyword(token, _tokenizer.keywords.global)) {\n        eatToken();\n\n        var _node9 = parseGlobal();\n\n        var _endLoc11 = getEndLoc();\n\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n        return t.withLoc(_node9, _endLoc11, startLoc);\n      }\n\n      if (isKeyword(token, _tokenizer.keywords.type)) {\n        eatToken();\n\n        var _node10 = parseType();\n\n        var _endLoc12 = getEndLoc();\n\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n        return t.withLoc(_node10, _endLoc12, startLoc);\n      }\n\n      if (isKeyword(token, _tokenizer.keywords.start)) {\n        eatToken();\n\n        var _node11 = parseStart();\n\n        var _endLoc13 = getEndLoc();\n\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n        return t.withLoc(_node11, _endLoc13, startLoc);\n      }\n\n      if (isKeyword(token, _tokenizer.keywords.elem)) {\n        eatToken();\n\n        var _node12 = parseElem();\n\n        var _endLoc14 = getEndLoc();\n\n        eatTokenOfType(_tokenizer.tokens.closeParen);\n        return t.withLoc(_node12, _endLoc14, startLoc);\n      }\n\n      var instruction = parseFuncInstr();\n      var endLoc = getEndLoc();\n      maybeIgnoreComment();\n\n      if (_typeof(instruction) === \"object\") {\n        if (typeof token !== \"undefined\") {\n          eatTokenOfType(_tokenizer.tokens.closeParen);\n        }\n\n        return t.withLoc(instruction, endLoc, startLoc);\n      }\n    }\n\n    if (token.type === _tokenizer.tokens.comment) {\n      var _startLoc = getStartLoc();\n\n      var builder = token.opts.type === \"leading\" ? t.leadingComment : t.blockComment;\n\n      var _node13 = builder(token.value);\n\n      eatToken(); // comment\n\n      var _endLoc15 = getEndLoc();\n\n      return t.withLoc(_node13, _endLoc15, _startLoc);\n    }\n\n    throw function () {\n      return new Error(\"\\n\" + (0, _helperCodeFrame.codeFrameFromSource)(source, token.loc) + \"\\n\" + \"Unknown token\" + \", given \" + tokenToString(token));\n    }();\n  }\n\n  var body = [];\n\n  while (current < tokensList.length) {\n    body.push(walk());\n  }\n\n  return t.program(body);\n}"},"sourceMaps":null,"error":null,"hash":"3d50feaecc25a93db2097b981cf90922","cacheData":{"env":{}}}